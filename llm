#!/bin/bash
# llm - Simple CLI for LLM interactions via llm_wrapper.py
# Works great with pak4 semantic compression!
#
# Examples:
#   llm "What is the meaning of life?"
#   pak4 project/ -c4 | llm "Analyze this codebase"
#   llm "Generate docs from:" < semantic.pak

VERSION="1.0.0"
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
LLM_WRAPPER_PY="$SCRIPT_DIR/llm_wrapper.py"

# Colors for pretty output
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

usage() {
    cat << EOF
${CYAN}llm v$VERSION${NC} - Simple LLM CLI using OpenRouter

${YELLOW}USAGE:${NC}
  llm "your prompt here"                 # Direct prompt
  echo "text" | llm "analyze this"       # Pipe input
  pak4 project/ -c4 | llm "review code"  # Perfect with pak4!
  llm -h                                 # This help

${YELLOW}EXAMPLES:${NC}
  ${GREEN}# Basic usage${NC}
  llm "What is Python?"

  ${GREEN}# Code analysis with pak4${NC}
  pak4 src/ -c4 | llm "Find potential bugs"
  pak4 . -c4 | llm "Generate documentation"
  pak4 project/ -c4 | llm "Suggest improvements"

  ${GREEN}# With files${NC}
  llm "Explain this code:" < script.py
  cat README.md | llm "Make this more engaging"

${YELLOW}ENVIRONMENT:${NC}
  OPENROUTER_API_KEY    Required OpenRouter API key
  SEMANTIC_MODEL        LLM model (default: anthropic/claude-3-haiku:beta)

${YELLOW}NOTES:${NC}
  â€¢ Requires llm_wrapper.py in same directory
  â€¢ Supports piped input for seamless pak4 integration
  â€¢ Output is clean for further processing
EOF
}

# Check dependencies
check_deps() {
    if [ ! -f "$LLM_WRAPPER_PY" ]; then
        echo -e "${RED}Error:${NC} llm_wrapper.py not found at: $LLM_WRAPPER_PY" >&2
        echo "Please ensure llm_wrapper.py is in the same directory as llm script." >&2
        exit 1
    fi

    if ! python3 -c "import requests, dotenv" 2>/dev/null; then
        echo -e "${RED}Error:${NC} Missing Python dependencies." >&2
        echo "Install with: pip install requests python-dotenv" >&2
        exit 1
    fi

    if [ -z "$OPENROUTER_API_KEY" ]; then
        echo -e "${YELLOW}Warning:${NC} OPENROUTER_API_KEY not set." >&2
        echo "Set it in .env file or environment variable." >&2
        # Don't exit - let llm_wrapper.py handle the error
    fi
}

# Build prompt from arguments and stdin
build_prompt() {
    local prompt=""

    # Get prompt from arguments
    if [ $# -gt 0 ]; then
        prompt="$*"
    fi

    # Check if there's input from pipe/stdin
    if [ ! -t 0 ]; then
        local stdin_content
        stdin_content=$(cat)

        if [ -n "$stdin_content" ]; then
            if [ -n "$prompt" ]; then
                # Combine prompt with stdin
                prompt="$prompt

$stdin_content"
            else
                # Use only stdin if no prompt provided
                prompt="$stdin_content"
            fi
        fi
    fi

    echo "$prompt"
}

# Call LLM via Python wrapper
call_llm() {
    local prompt="$1"

    if [ -z "$prompt" ]; then
        echo -e "${RED}Error:${NC} No prompt provided." >&2
        usage >&2
        exit 1
    fi

    # Create a simple Python script to call llm_wrapper
    python3 -c "
import sys
import os
sys.path.insert(0, '$SCRIPT_DIR')

try:
    from llm_wrapper import llm_call

    # Load .env if available
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass

    prompt = '''$prompt'''
    messages = [{'role': 'user', 'content': prompt}]

    response, success = llm_call(messages)

    if success:
        print(response)
        sys.exit(0)
    else:
        print('Error: LLM call failed. Check your API key and connection.', file=sys.stderr)
        sys.exit(1)

except ImportError as e:
    print(f'Error: Failed to import llm_wrapper: {e}', file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f'Error: {e}', file=sys.stderr)
    sys.exit(1)
"
}

# Main function
main() {
    # Handle help and version
    case "${1:-}" in
        -h|--help|help)
            usage
            exit 0
            ;;
        -v|--version)
            echo "llm v$VERSION"
            exit 0
            ;;
        "")
            # No arguments, check if there's stdin
            if [ -t 0 ]; then
                echo -e "${RED}Error:${NC} No prompt provided." >&2
                echo "Use: llm \"your prompt\" or pipe input: echo \"text\" | llm \"analyze\"" >&2
                exit 1
            fi
            ;;
    esac

    # Check dependencies
    check_deps

    # Build the full prompt from args and stdin
    prompt=$(build_prompt "$@")

    # Show a subtle indicator when processing (to stderr so it doesn't affect piped output)
    if [ -t 1 ]; then  # Only if stdout is a terminal
        echo -e "${BLUE}ðŸ¤–${NC} Thinking..." >&2
    fi

    # Call LLM
    call_llm "$prompt"
}

# Load .env if exists
if [ -f "$SCRIPT_DIR/.env" ]; then
    set -a
    source "$SCRIPT_DIR/.env"
    set +a
fi

# Run main function
main "$@"