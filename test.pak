{
  "metadata": {
    "pak_format_version": "4.2.0-refactored",
    "archive_uuid": "XCKU",
    "creation_timestamp_utc": "2025-06-19T07:52:34.676426Z",
    "source_tool_version": "pak_core_refactored_v_unknown",
    "compression_level_setting": "semantic",
    "max_tokens_setting": 0,
    "total_files": 19,
    "total_original_size_bytes": 235743,
    "total_compressed_size_bytes": 63482,
    "total_estimated_tokens": 21153
  },
  "files": [
    {
      "path": "CLAUDE.md",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: CLAUDE.md (11598 bytes, markdown)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"CLAUDE.md\",\n  \"file_type\": \"markdown\",\n  \"overall_purpose\": \"Provides guidance to Claude Code when working with the 'pak' semantic compression tool family, detailing its architecture, usage, and testing procedures.\",\n  \"key_components\": {\n    \"imports_dependencies\": [],\n    \"classes\": [\n      {\n        \"name\": \"SmartArchiver\",\n        \"purpose\": \"Main orchestration for file collection, compression, and archive generation.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"CompressionStrategy\",\n        \"purpose\": \"Abstract base class for compression strategies.\",\n        \"key_methods\": [\n          \"compress(content, file_path, language): Compresses content based on the strategy.\"\n        ],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"ASTCompression\",\n        \"purpose\": \"Uses tree-sitter for syntax-aware compression.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"SemanticCompression\",\n        \"purpose\": \"Leverages LLM for intelligent content reduction.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"FilePrioritizer\",\n        \"purpose\": \"Assigns importance scores to files for smart compression.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"LanguageDetector\",\n        \"purpose\": \"Maps file extensions to language identifiers.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      }\n    ],\n    \"functions_methods\": [],\n    \"data_structures\": [\n      \"__PAK_UUID__: UUID marker for archive format.\",\n      \"__PAK_FILE_<uuid>_START__: Archive file metadata marker.\",\n      \"__PAK_DATA_<uuid>_START__: Archive data content marker.\",\n      \"EXTENSION_MAP: Maps file extensions to language identifiers (in LanguageDetector).\",\n      \"LOW_PRIORITY_PATTERNS: Patterns for file exclusions (in FilePrioritizer).\",\n      \"CRITICAL_FILENAMES: Files with special importance (in FilePrioritizer).\"\n    ],\n    \"configuration\": [\n      \"OPENROUTER_API_KEY: API key for LLM-based semantic compression (in .env file).\",\n      \"SEMANTIC_COMPRESSOR_PATH: Path to semantic_compressor.py for LLM compression.\",\n      \"PAK_DEBUG: Enable detailed debug output.\",\n      \"PAK_QUIET: Suppress stderr messages during compression.\"\n    ]\n  },\n  \"core_logic_flow\": \"The 'pak' tool family packs files, compresses them using various strategies (None, Light, Medium, Aggressive, Semantic), and stores them in a custom archive format. It also supports method-level diff generation, verification, and application. The main Python CLI (pak.py) orchestrates these operations, utilizing modules for compression, language detection, and archive management. Standalone executables are built using PyInstaller.\",\n  \"critical_reconstruction_details\": [\n    \"Custom text-based archive format with UUID markers for file and data sections.\",\n    \"Method-level diff system for granular code changes.\",\n    \"Compression Strategy Pattern (ABC) for different compression levels.\",\n    \"File prioritization based on importance scores.\",\n    \"Integration with external LLM services via llm_wrapper.py.\",\n    \"Testing architecture combining integration and unit tests, including mock LLM testing.\",\n    \"Standalone executable build process using PyInstaller and Poetry.\"\n  ],\n  \"external_interactions\": [\n    \"Interacts with external LLM services via API calls (llm_wrapper.py).\",\n    \"Uses tree-sitter for AST analysis (optional dependency).\",\n    \"Uses Poetry for dependency management.\",\n    \"Uses PyInstaller for creating standalone executables.\"\n  ]\n}",
      "original_size_bytes": 11598,
      "compressed_size_bytes": 3777,
      "estimated_tokens": 1259,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 3.0706910246227164,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T07:36:22.182739Z"
    },
    {
      "path": "PAKDIFF.md",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: PAKDIFF.md (11497 bytes, markdown)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"PAKDIFF.md\",\n  \"file_type\": \"markdown\",\n  \"overall_purpose\": \"This document specifies the pak4diff format, a method-level diff format designed for granular code modifications. It outlines the format's structure, operation types, language-specific signature matching, and best practices for its use.\",\n  \"key_components\": {\n    \"imports_dependencies\": [],\n    \"classes\": [],\n    \"functions_methods\": [],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The pak4diff format defines a structured way to represent code changes at the method level. It uses a simple schema with FILE, FIND_METHOD, UNTIL_EXCLUDE, and REPLACE_WITH sections to specify modifications, additions, and deletions. The document details how to apply these diffs, including handling special characters and best practices for generating and applying them, especially when using LLMs.\",\n  \"critical_reconstruction_details\": \"The core of pak4diff is its method-level diffing approach. Key details include: the use of method signatures for identification, the UNTIL_EXCLUDE tag for defining the scope of changes, the REPLACE_WITH section for the modified code, and the handling of special characters (backticks) to avoid markdown conflicts. The document also provides language-specific signature formats for Python, JavaScript, and Java.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 11497,
      "compressed_size_bytes": 1519,
      "estimated_tokens": 506,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 7.5687952600395,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.083194Z"
    },
    {
      "path": "README-BIS.md",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: README-BIS.md (11084 bytes, markdown)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"README-BIS.md\",\n  \"file_type\": \"markdown\",\n  \"overall_purpose\": \"This README documents pak4.py, a command-line tool designed to streamline the process of interacting with Large Language Models (LLMs) for code manipulation tasks by efficiently packaging and unpacking code files.\",\n  \"key_components\": {\n    \"imports_dependencies\": [],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"pak4.py\",\n        \"signature\": null,\n        \"purpose\": \"A command-line tool for packaging and unpacking code files for use with LLMs, including compression and token management.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": [\n      \"Compression levels: light, medium, aggressive, smart\",\n      \"File extensions for filtering\",\n      \"Maximum token limit for smart compression\",\n      \"Output directory for unpacking\"\n    ]\n  },\n  \"core_logic_flow\": \"pak4.py takes code files as input, compresses them into a `.pak` file, which is then used as context for an LLM. The LLM processes the context and generates modified code. pak4.py then unpacks the LLM's response, saving the modified code back into the original files or a specified output directory. The tool supports various compression levels and token management to optimize the context sent to the LLM.\",\n  \"critical_reconstruction_details\": \"The `.pak` file format is designed to be LLM-native, including file paths, language, size, compression method, and the compressed code itself. The smart compression level prioritizes important files and manages token limits. AST-based compression is used for advanced compression, falling back to textual compression if AST parsing fails.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 11084,
      "compressed_size_bytes": 1852,
      "estimated_tokens": 617,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 5.984881209503239,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.083194Z"
    },
    {
      "path": "README.md",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: README.md (9162 bytes, markdown)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"README.md\",\n  \"file_type\": \"markdown\",\n  \"overall_purpose\": \"This README documents Pak, a command-line tool designed to compress and package code files for use with Large Language Models (LLMs), streamlining the process of providing context to LLMs and extracting modified code.\",\n  \"key_components\": {\n    \"imports_dependencies\": [],\n    \"classes\": [],\n    \"functions_methods\": [],\n    \"data_structures\": [],\n    \"configuration\": [\n      \"Environment variables: `OPENROUTER_API_KEY` (for semantic compression), `PAK_DEBUG`, `PAK_QUIET`.\",\n      \"Configuration files: `.env` (for environment variables), `pyproject.toml` (for Python dependencies).\"\n    ]\n  },\n  \"core_logic_flow\": \"Pak takes a set of files as input, compresses them using various strategies (raw, whitespace removal, comment removal, AST-based, semantic), and packages them into an archive. It also provides functionality to extract the archive, apply method-level diffs, and filter files. The tool prioritizes files and manages token budgets to fit within LLM context windows.\",\n  \"critical_reconstruction_details\": \"The tool uses AST analysis and LLM-based compression for semantic understanding of code. Method-level diffs are generated and applied. The archive format is custom and JSON-based. The `install.sh` script handles standalone executable creation and includes a fix for shared library issues in certain environments.\",\n  \"external_interactions\": [\n    \"Interacts with external LLM services (e.g., OpenRouter) for semantic compression via the `OPENROUTER_API_KEY` environment variable.\",\n    \"Uses Poetry for dependency management.\"\n  ]\n}",
      "original_size_bytes": 9162,
      "compressed_size_bytes": 1771,
      "estimated_tokens": 590,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 5.173348390739695,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T07:35:56.278351Z"
    },
    {
      "path": "README_PAK3.md",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: README_PAK3.md (7583 bytes, markdown)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"README_PAK3.md\",\n  \"file_type\": \"markdown\",\n  \"overall_purpose\": \"This README documents Pak3, a command-line utility designed to archive code projects for Large Language Models (LLMs) by employing semantic Abstract Syntax Tree (AST) analysis for intelligent compression, optimizing token usage.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"tree-sitter\",\n      \"tree-sitter-languages\",\n      \"tree-sitter-python (for Python)\",\n      \"bash (for orchestration)\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [],\n    \"data_structures\": [],\n    \"configuration\": [\n      \"Compression Levels: none, light, medium, aggressive, smart\",\n      \"Token Budget Management: --max-tokens <integer>\",\n      \"Auto-installation of dependencies: --disable-auto-install\"\n    ]\n  },\n  \"core_logic_flow\": \"Pak3 analyzes code projects using AST (if supported) or textual algorithms to compress files into a `.pak` archive. It offers different compression levels, including a 'smart' mode that adapts to token budgets and file importance. The utility prioritizes critical files and supports automatic dependency installation. The main script orchestrates the process, utilizing AST compression engines and specialized extractors for different languages.\",\n  \"critical_reconstruction_details\": \"The core innovation is the use of AST analysis via tree-sitter for semantic compression. This involves intelligent comment removal, API surface extraction, and structural compression. The 'smart' compression mode uses a prioritization algorithm based on file importance, token budget, and content type. The archive format includes metadata such as compression mode, AST support status, and token limits. The utility automatically falls back to textual compression if AST is not supported.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 7583,
      "compressed_size_bytes": 1970,
      "estimated_tokens": 656,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 3.849238578680203,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "ast_helper.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: ast_helper.py (10720 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"ast_helper.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This script acts as a helper for code analysis, primarily using tree-sitter to parse code and extract information based on compression levels. It's designed to be called by another tool (pak3) to process code files.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"sys\",\n      \"re\",\n      \"tree_sitter\",\n      \"tree_sitter_languages\",\n      \"tree_sitter_python\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"get_language_object\",\n        \"signature\": \"get_language_object(lang_name_short)\",\n        \"purpose\": \"Loads and returns a tree-sitter Language object based on a short language name (e.g., 'python', 'javascript').\"\n      },\n      {\n        \"name\": \"extract_api_only\",\n        \"signature\": \"extract_api_only(tree, source_bytes)\",\n        \"purpose\": \"Extracts API-level elements (imports, class/function headers) from the AST.  Currently a stub, with Python-specific logic.\"\n      },\n      {\n        \"name\": \"extract_signatures_preview\",\n        \"signature\": \"extract_signatures_preview(tree, source_bytes)\",\n        \"purpose\": \"Extracts signatures and some context.  Currently a stub.\"\n      },\n      {\n        \"name\": \"extract_clean_code\",\n        \"signature\": \"extract_clean_code(tree, source_bytes)\",\n        \"purpose\": \"Removes comments and potentially whitespace.  Currently a stub.\"\n      },\n      {\n        \"name\": \"main\",\n        \"signature\": \"main()\",\n        \"purpose\": \"Main function that handles command-line arguments, file reading, language loading, AST parsing, and content extraction based on the specified compression level.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The script takes a file path, compression level, and language name as command-line arguments. It reads the file, loads the appropriate tree-sitter language grammar, parses the code into an AST, and then calls a function (extract_api_only, extract_signatures_preview, or extract_clean_code) to extract information based on the compression level. The extracted content is then printed to standard output. If any error occurs during processing, it attempts to print the original content as a fallback.\",\n  \"critical_reconstruction_details\": \"The script relies heavily on tree-sitter for parsing. The `extract_api_only` function demonstrates the general pattern for AST traversal, but the specific node types and extraction logic are highly language-dependent. The script includes error handling and a fallback mechanism to print the original content if parsing fails, allowing the calling tool (pak3) to still capture *something*. The language mapping in `get_language_object` is crucial for supporting different languages. The script uses stdout for output, which is important for the calling tool to capture the results.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 10720,
      "compressed_size_bytes": 3059,
      "estimated_tokens": 1019,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 3.5044132069303693,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "hook-tiktoken.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: hook-tiktoken.py (96 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"hook-tiktoken.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file is a PyInstaller hook that collects data files for the 'tiktoken' library, making them available during the packaging process.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"PyInstaller.utils.hooks\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"collect_data_files\",\n        \"signature\": \"collect_data_files(module_name: str)\",\n        \"purpose\": \"Collects data files associated with a given module for PyInstaller.\"\n      }\n    ],\n    \"data_structures\": [\n      \"datas: Stores the result of collecting data files for 'tiktoken'.\"\n    ],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The file imports the `collect_data_files` function from PyInstaller's hooks. It then calls this function, passing 'tiktoken' as the module name, and stores the returned data files in the `datas` variable. This ensures that tiktoken's data files are included when the application is packaged.\",\n  \"critical_reconstruction_details\": \"The specific implementation of `collect_data_files` is not provided, but the file relies on it to identify and include the necessary data files for the 'tiktoken' library during the PyInstaller build process.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 96,
      "compressed_size_bytes": 1440,
      "estimated_tokens": 480,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 0.06666666666666667,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T18:47:17.468550Z"
    },
    {
      "path": "llm_wrapper.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: llm_wrapper.py (3537 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"llm_wrapper.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"Provides a simplified wrapper for interacting with an LLM API (OpenRouter). It handles API calls, error handling, and includes a connection test.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"requests\",\n      \"json\",\n      \"logging\",\n      \"typing\",\n      \"dotenv (optional)\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"llm_call\",\n        \"signature\": \"llm_call(messages: List[Dict[str, str]], model: Optional[str] = None, max_tokens: int = 4000, temperature: float = 0.1) -> Tuple[str, bool]\",\n        \"purpose\": \"Makes a POST request to the OpenRouter API with the provided messages and parameters. Returns the LLM's response and a success flag.\"\n      },\n      {\n        \"name\": \"test_llm_connection\",\n        \"signature\": \"test_llm_connection() -> bool\",\n        \"purpose\": \"Tests the LLM connection by sending a simple prompt and checking the response.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": [\n      \"OPENROUTER_API_KEY (required, from environment)\",\n      \"SEMANTIC_MODEL (optional, defaults to anthropic/claude-3-haiku:beta)\",\n      \"OPENROUTER_APP_URL (optional, defaults to http://localhost)\",\n      \"OPENROUTER_APP_TITLE (optional, defaults to Pak4SemanticCompressor)\",\n      \"max_tokens (default: 4000)\",\n      \"temperature (default: 0.1)\"\n    ]\n  },\n  \"core_logic_flow\": \"The script first attempts to load environment variables from a .env file. Then, it defines the `llm_call` function, which constructs a payload for the OpenRouter API, sends a POST request, and handles the response and potential errors. The `test_llm_connection` function uses `llm_call` to verify the connection. Finally, the script calls `test_llm_connection` and prints the result.\",\n  \"critical_reconstruction_details\": [\n    \"Uses the OpenRouter API for LLM interaction.\",\n    \"Error handling includes logging and returning error messages.\",\n    \"Includes a connection test for verifying API access.\",\n    \"Uses `requests` library for HTTP requests.\",\n    \"Handles JSON decoding errors.\"\n  ],\n  \"external_interactions\": [\n    \"Interacts with the OpenRouter API (https://openrouter.ai/api/v1/chat/completions).\",\n    \"Reads environment variables (OPENROUTER_API_KEY, SEMANTIC_MODEL, OPENROUTER_APP_URL, OPENROUTER_APP_TITLE) for configuration.\"\n  ]\n}",
      "original_size_bytes": 3537,
      "compressed_size_bytes": 2543,
      "estimated_tokens": 847,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 1.3908769170271333,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "pak.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak.py (20936 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file is the main command-line interface (CLI) entry point for the 'pak' utility, a tool for archiving files with semantic compression and method diff support.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"argparse\",\n      \"datetime\",\n      \"os\",\n      \"re\",\n      \"sys\",\n      \"pathlib\",\n      \"pak_utils\",\n      \"pak_analyzer\",\n      \"pak_compressor\",\n      \"pak_differ\",\n      \"pak_archive_manager\",\n      \"requests\",\n      \"dotenv\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"PakArchive\",\n        \"purpose\": \"Manages the creation, listing, extraction, and verification of archives.\",\n        \"key_methods\": [\n          \"add_file(file_path, content): Adds a file to the archive.\",\n          \"create_archive(output_path): Creates the archive file.\",\n          \"list_archive(archive_file, detailed, file_path_pattern, quiet): Lists archive contents.\",\n          \"extract_archive(archive_file, outdir, pattern, quiet): Extracts archive contents.\",\n          \"verify_archive(archive_file, quiet): Verifies archive integrity.\"\n        ],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"Compressor\",\n        \"purpose\": \"Handles file compression based on specified levels.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"CacheManager\",\n        \"purpose\": \"Manages caching for compression operations.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"MethodDiffManager\",\n        \"purpose\": \"Manages method-level diff extraction and application.\",\n        \"key_methods\": [\n          \"extract_diff(file_paths, quiet): Extracts method-level differences between files.\"\n        ],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"PythonAnalyzer\",\n        \"purpose\": \"Analyzes Python code for compression.\",\n        \"key_methods\": [],\n        \"key_attributes\": []\n      }\n    ],\n    \"functions_methods\": [\n      {\n        \"name\": \"check_dependencies\",\n        \"signature\": \"check_dependencies(quiet=False)\",\n        \"purpose\": \"Checks for required dependencies and configuration, including the presence of a .env file and Python packages.\"\n      },\n      {\n        \"name\": \"load_env\",\n        \"signature\": \"load_env()\",\n        \"purpose\": \"Loads environment variables from a .env file.\"\n      },\n      {\n        \"name\": \"test_llm_connection\",\n        \"signature\": \"test_llm_connection()\",\n        \"purpose\": \"Tests the LLM connection for semantic compression.\"\n      },\n      {\n        \"name\": \"auto_output_name\",\n        \"signature\": \"auto_output_name(targets, compression_level=\\\"medium\\\")\",\n        \"purpose\": \"Generates an automatic output filename based on targets and compression level.\"\n      },\n      {\n        \"name\": \"normalize_compression_level\",\n        \"signature\": \"normalize_compression_level(level)\",\n        \"purpose\": \"Normalizes compression level input to standard names.\"\n      },\n      {\n        \"name\": \"normalize_extensions\",\n        \"signature\": \"normalize_extensions(ext_list)\",\n        \"purpose\": \"Normalizes file extensions from various input formats.\"\n      },\n      {\n        \"name\": \"show_usage\",\n        \"signature\": \"show_usage()\",\n        \"purpose\": \"Displays comprehensive usage information.\"\n      },\n      {\n        \"name\": \"parse_legacy_args\",\n        \"signature\": \"parse_legacy_args(args)\",\n        \"purpose\": \"Parses legacy bash pak arguments for backward compatibility.\"\n      },\n      {\n        \"name\": \"main\",\n        \"signature\": \"main()\",\n        \"purpose\": \"The main function that parses arguments, determines the command to execute, and calls the appropriate function.\"\n      },\n      {\n        \"name\": \"execute_pack_command\",\n        \"signature\": \"execute_pack_command(args, compression_level, extensions)\",\n        \"purpose\": \"Executes the pack command, collecting files, creating the archive, and writing it to the output.\"\n      },\n      {\n        \"name\": \"execute_list_command\",\n        \"signature\": \"execute_list_command(args, command)\",\n        \"purpose\": \"Executes the list or list-detailed command, listing archive contents.\"\n      },\n      {\n        \"name\": \"execute_extract_command\",\n        \"signature\": \"execute_extract_command(args)\",\n        \"purpose\": \"Executes the extract command, extracting archive contents.\"\n      },\n      {\n        \"name\": \"execute_verify_command\",\n        \"signature\": \"execute_verify_command(args)\",\n        \"purpose\": \"Executes the verify command, verifying archive integrity.\"\n      },\n      {\n        \"name\": \"execute_diff_command\",\n        \"signature\": \"execute_diff_command(args)\",\n        \"purpose\": \"Executes the extract-diff command, extracting method-level differences.\"\n      },\n      {\n        \"name\": \"execute_verify_diff_command\",\n        \"signature\": \"execute_verify_diff_command(args)\",\n        \"purpose\": \"Executes the verify-diff command, verifying method diff file.\"\n      },\n      {\n        \"name\": \"execute_apply_diff_command\",\n        \"signature\": \"execute_apply_diff_command(args)\",\n        \"purpose\": \"Executes the apply-diff command, applying method diff to files.\"\n      }\n    ],\n    \"data_structures\": [\n      \"VERSION: String representing the version of the pak utility.\",\n      \"suffix_map: Dictionary mapping compression levels to filename suffixes.\"\n    ],\n    \"configuration\": [\n      \"Compression level (0-4, s, or full names) via -c or --compression-level.\",\n      \"Maximum token limit via -m or --max-tokens.\",\n      \"File extensions via -t or --types.\",\n      \"Output file via -o or --output.\",\n      \"Output directory for extraction via -d or --outdir.\",\n      \"Regex pattern for filtering files via -p or --pattern.\",\n      \"Quiet mode via -q or --quiet.\",\n      \".env file for semantic compression (OPENROUTER_API_KEY).\"\n    ]\n  },\n  \"core_logic_flow\": \"The script parses command-line arguments to determine the desired action (pack, list, extract, verify, diff). It then normalizes inputs like compression level and file extensions. Based on the action, it calls the appropriate function (e.g., execute_pack_command, execute_extract_command). The pack command collects files, compresses them based on the specified level, and creates an archive. Other commands perform actions on existing archives or method diff files.\",\n  \"critical_reconstruction_details\": [\n    \"The script supports multiple compression levels, including semantic compression which relies on an LLM.\",\n    \"Legacy argument parsing is implemented for backward compatibility.\",\n    \"Method diff functionality allows extracting and applying method-level changes between files.\",\n    \"Automatic output filename generation based on targets and compression level.\",\n    \"Uses external modules for file collection, analysis, compression, diffing, and archive management.\"\n  ],\n  \"external_interactions\": [\n    \"Interacts with the file system to read and write files.\",\n    \"May interact with an LLM (OpenRouter) for semantic compression (requires OPENROUTER_API_KEY in .env).\",\n    \"Uses external modules for compression, analysis, and archive management.\"\n  ]\n}",
      "original_size_bytes": 20936,
      "compressed_size_bytes": 7285,
      "estimated_tokens": 2428,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 2.873850377487989,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T06:54:36.117083Z"
    },
    {
      "path": "pak_analyzer.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak_analyzer.py (23316 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak_analyzer.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file analyzes code structure for multiple programming languages using Abstract Syntax Trees (ASTs). It provides compression and comparison functionalities, leveraging both the built-in `ast` module for Python and an external `ast_helper` binary for other languages.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"ast\",\n      \"re\",\n      \"typing: List, Dict, Any, Optional, Union\",\n      \"subprocess\",\n      \"tempfile\",\n      \"os\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"MultiLanguageAnalyzer\",\n        \"purpose\": \"Analyzes code structure and provides compression and comparison features.\",\n        \"key_methods\": [\n          \"compress_with_ast(content: str, language: str, level: str = \\\"aggressive\\\") -> str: Compresses code using AST analysis, with fallback to regex.\",\n          \"_compress_via_ast_helper(content: str, language: str, level: str) -> str: Calls an external ast_helper binary for AST-based compression.\",\n          \"_fallback_compression(content: str, language: str, level: str) -> str: Provides regex-based compression as a fallback.\",\n          \"_format_python_structure(structure: Dict[str, Any]) -> str: Formats Python AST structure into a compressed code representation.\",\n          \"_get_node_source_segment(node: ast.AST, content_lines: List[str]) -> str: Extracts the source code segment for a given AST node.\",\n          \"extract_structure(content: str) -> Dict[str, Any]: Extracts Python code structure (imports, functions, classes, variables) using AST.\",\n          \"extract_methods(content: str) -> List[Dict[str, Any]]: Extracts individual functions and methods with their source code.\",\n          \"compare_methods(old_content: str, new_content: str) -> List[Dict[str, Any]]: Compares methods between two Python code strings and returns diffs.\"\n        ],\n        \"key_attributes\": []\n      }\n    ],\n    \"functions_methods\": [],\n    \"data_structures\": [\n      \"structure: Dict[str, Any] - Used to store extracted code structure (imports, functions, classes, variables) in extract_structure.\",\n      \"methods_details: List[Dict[str, Any]] - Stores extracted method details (name, source, signature) in extract_methods.\",\n      \"diff_results: List[Dict[str, Any]] - Stores the diff results (added, removed, modified) in compare_methods.\"\n    ],\n    \"configuration\": [\n      \"AST_HELPER_BIN: Environment variable specifying the path to the ast_helper binary. Defaults to 'ast_helper'.\"\n    ]\n  },\n  \"core_logic_flow\": \"The `MultiLanguageAnalyzer` class provides the core functionality. `compress_with_ast` is the main entry point, which dispatches to Python-specific AST compression or uses an external `ast_helper` binary for other languages. If the binary fails, it falls back to regex-based compression. The `extract_structure` method parses Python code using the `ast` module to extract structural information. `extract_methods` extracts function/method details. `compare_methods` compares methods between two code versions.\",\n  \"critical_reconstruction_details\": \"The `ast_helper` binary is crucial for non-Python language support. The fallback regex compression provides basic functionality. The `extract_structure` method's logic for handling imports, functions, classes, and variables is essential. The method comparison logic in `compare_methods` uses source normalization (whitespace stripping) for more robust diffing. The handling of decorators and multi-line nodes in `_get_node_source_segment` is important for accurate source extraction.\",\n  \"external_interactions\": [\n    \"Uses `subprocess` to execute the `ast_helper` binary. The `ast_helper` binary is assumed to be in the same directory or in the PATH.\",\n    \"Reads and writes temporary files using `tempfile` for passing code content to the `ast_helper`.\"\n  ]\n}",
      "original_size_bytes": 23316,
      "compressed_size_bytes": 4009,
      "estimated_tokens": 1336,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 5.815914193065602,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T19:49:25.753424Z"
    },
    {
      "path": "pak_archive_manager.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak_archive_manager.py (19216 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak_archive_manager.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"Manages the creation, extraction, listing, and verification of .pak archives, which are JSON-based containers for files, including compression and metadata.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"sys\",\n      \"json\",\n      \"uuid\",\n      \"datetime\",\n      \"string\",\n      \"random\",\n      \"re\",\n      \"pathlib\",\n      \"typing\",\n      \"pak_compressor.py (Compressor, CacheManager)\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"PakArchive\",\n        \"purpose\": \"Handles .pak archive operations: creation, extraction, listing, and verification.\",\n        \"key_methods\": [\n          \"__init__(self, compression_level: str = \\\"medium\\\", max_tokens: int = 0, quiet: bool = False): Initializes the archive with compression settings and internal data structures.\",\n          \"_generate_short_uuid(self) -> str: Generates a short, random UUID.\",\n          \"_log(self, message: str, is_error: bool = False): Logs messages to stderr or stdout based on quiet setting.\",\n          \"set_cache_manager(self, cache_manager: CacheManager): Sets a CacheManager for compression.\",\n          \"add_file(self, file_path: str, content: str, importance: int = 0): Adds a file to the archive, compressing the content.\",\n          \"create_archive(self, output_file_path: Optional[str] = None) -> Optional[str]: Finalizes the archive and writes it to a JSON file or returns the JSON string.\",\n          \"_load_archive_json_data(archive_file_path: str, quiet: bool = False) -> Dict[str, Any]: Loads and validates archive JSON data.\",\n          \"extract_archive(archive_file_path: str, output_base_dir: str, file_path_pattern: Optional[str] = None, quiet: bool = False): Extracts files from the archive.\",\n          \"list_archive(archive_file_path: str, detailed: bool = False, file_path_pattern: Optional[str] = None, quiet: bool = False): Lists the contents of the archive.\",\n          \"verify_archive(archive_file_path: str, quiet: bool = False) -> bool: Verifies the integrity of the archive.\"\n        ],\n        \"key_attributes\": [\n          \"PAK_FORMAT_VERSION: str = \\\"4.2.0-refactored\\\"\",\n          \"compression_level: str\",\n          \"max_tokens: int\",\n          \"files_data: List[Dict[str, Any]]\",\n          \"archive_uuid: str\",\n          \"cache_manager: Optional[CacheManager]\",\n          \"quiet: bool\",\n          \"total_original_size_bytes: int\",\n          \"total_compressed_size_bytes: int\",\n          \"total_estimated_tokens: int\"\n        ]\n      }\n    ],\n    \"functions_methods\": [],\n    \"data_structures\": [\n      \"files_data: List of dictionaries, each representing a file in the archive. Each dictionary contains file metadata (path, content, sizes, compression details, etc.).\",\n      \"archive_metadata: Dictionary containing archive-level metadata (version, UUID, timestamps, compression settings, totals).\"\n    ],\n    \"configuration\": [\n      \"compression_level: 'light', 'medium', or other values supported by the Compressor.\",\n      \"max_tokens: Integer, intended for token budgeting (not fully implemented).\",\n      \"quiet: Boolean, controls logging verbosity.\"\n    ]\n  },\n  \"core_logic_flow\": \"The PakArchive class manages the lifecycle of .pak archives.  Files are added to the archive using `add_file`, which compresses the content using a `Compressor` instance.  The `create_archive` method finalizes the archive structure and writes it to a JSON file.  Static methods `extract_archive`, `list_archive`, and `verify_archive` provide functionality for interacting with existing archive files.  Error handling and logging are integrated throughout the process.\",\n  \"critical_reconstruction_details\": [\n    \"Uses a JSON-based format for the archive, including metadata and a list of file entries.\",\n    \"File paths are normalized to POSIX-style ('/') within the archive.\",\n    \"Compression is handled by a separate `Compressor` class (imported from `pak_compressor.py`), allowing for different compression methods and levels.\",\n    \"The `extract_archive` method includes a security check to prevent writing files outside the intended output directory.\",\n    \"The `list_archive` method outputs to stdout, while errors are directed to stderr.\",\n    \"The `verify_archive` method performs structural and data integrity checks on the archive JSON.\"\n  ],\n  \"external_interactions\": [\n    \"Imports and uses the `Compressor` and `CacheManager` classes from `pak_compressor.py` for compression and caching.\",\n    \"Writes archive data to JSON files.\",\n    \"Reads archive data from JSON files.\",\n    \"Interacts with the file system for file operations (reading, writing, and directory creation).\",\n    \"Uses `datetime` for timestamps.\"\n  ]\n}",
      "original_size_bytes": 19216,
      "compressed_size_bytes": 4897,
      "estimated_tokens": 1632,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 3.9240351235450275,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "pak_compressor.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak_compressor.py (32540 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak_compressor.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file provides functionality for compressing file content at various levels, including semantic compression using an LLM. It utilizes caching to optimize performance.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"sys\",\n      \"json\",\n      \"re\",\n      \"hashlib\",\n      \"pathlib\",\n      \"typing\",\n      \"pak_analyzer (sibling module)\",\n      \"requests (optional)\",\n      \"dotenv (optional)\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"TokenCounter\",\n        \"purpose\": \"Counts tokens in a string using a simple heuristic.\",\n        \"key_methods\": [\n          \"count_tokens(content: str, file_type: str) -> int: Counts tokens based on character count.\"\n        ],\n        \"key_attributes\": []\n      },\n      {\n        \"name\": \"CacheManager\",\n        \"purpose\": \"Manages a cache for storing and retrieving compression results based on SHA-256 hashes.\",\n        \"key_methods\": [\n          \"__init__(archive_path_or_id: str, quiet: bool): Initializes the cache, loading from a file.\",\n          \"save_cache(): Saves the cache to a JSON file.\",\n          \"get_sha256(content: str) -> str: Calculates the SHA-256 hash of a string.\",\n          \"get_cached_compression(content: str, compression_level: str, model_info: Optional[str]) -> Optional[Dict[str, Any]]: Retrieves cached compression results.\",\n          \"cache_compression(content: str, compression_level: str, result: Dict[str, Any], model_info: Optional[str]): Caches compression results.\"\n        ],\n        \"key_attributes\": [\n          \"cache_file: Path to the cache file.\",\n          \"cache: Dictionary storing cached results.\",\n          \"quiet: Boolean indicating whether to suppress output.\"\n        ]\n      },\n      {\n        \"name\": \"SemanticCompressor\",\n        \"purpose\": \"Handles semantic compression using an LLM (OpenRouter).\",\n        \"key_methods\": [\n          \"__init__(quiet: bool): Initializes the compressor, loading API keys and model settings.\",\n          \"compress_content(content: str, file_path: str, file_type: str) -> Dict[str, Any]: Compresses content semantically using an LLM.\",\n          \"_build_compression_prompt(content: str, file_path: str, file_type: str) -> str: Builds the prompt for the LLM.\",\n          \"_call_llm_api(prompt: str) -> str: Calls the LLM API and returns the response.\",\n          \"_parse_compression_response(llm_response_text: str, file_path_for_log: str) -> Dict[str, Any]: Parses the LLM response, extracting the JSON.\"\n        ],\n        \"key_attributes\": [\n          \"api_key: API key for the LLM service.\",\n          \"model_name: Name of the LLM model.\",\n          \"api_base_url: Base URL for the LLM API.\",\n          \"timeout: Timeout for API requests.\",\n          \"max_tokens_response: Maximum tokens for the LLM response.\",\n          \"temperature: Temperature for the LLM.\",\n          \"quiet: Boolean indicating whether to suppress output.\"\n        ]\n      },\n      {\n        \"name\": \"Compressor\",\n        \"purpose\": \"Orchestrates the compression process at different levels.\",\n        \"key_methods\": [\n          \"__init__(cache_manager: Optional[CacheManager], quiet: bool): Initializes the compressor.\",\n          \"_detect_file_type(file_path: str) -> str: Detects the file type based on the file extension.\",\n          \"compress_content(content: str, file_path: str, compression_level: str) -> Dict[str, Any]: Compresses content based on the specified level.\"\n        ],\n        \"key_attributes\": [\n          \"cache_manager: Instance of CacheManager for caching results.\",\n          \"semantic_compressor: Instance of SemanticCompressor for semantic compression.\",\n          \"quiet: Boolean indicating whether to suppress output.\",\n          \"semantic_model_info: Model name used for semantic compression caching.\"\n        ]\n      }\n    ],\n    \"functions_methods\": [\n      {\n        \"name\": \"count_tokens\",\n        \"signature\": \"count_tokens(content: str, file_type: str = \\\"text\\\") -> int\",\n        \"purpose\": \"Counts tokens in a string using a simple heuristic (3 chars = 1 token).\"\n      }\n    ],\n    \"data_structures\": [\n      \"cache: Dictionary used by CacheManager to store compression results, keyed by a hash of the content and compression level.\",\n      \"type_map: Dictionary mapping file extensions to file types.\"\n    ],\n    \"configuration\": [\n      \"OPENROUTER_API_KEY: API key for OpenRouter (required for semantic compression).\",\n      \"SEMANTIC_MODEL: LLM model name (default: anthropic/claude-3-haiku-20240307).\",\n      \"PAK_OPENROUTER_API_BASE: OpenRouter API base URL.\",\n      \"PAK_LLM_TIMEOUT: LLM API timeout (default: 60 seconds).\",\n      \"PAK_LLM_MAX_TOKENS: Maximum tokens for LLM response (default: 2000).\",\n      \"PAK_LLM_TEMPERATURE: LLM temperature (default: 0.1).\",\n      \"PAK_CACHE_DIR: Directory for caching compression results (defaults to ~/.cache/pak_tool_cache).\"\n    ]\n  },\n  \"core_logic_flow\": \"The script defines classes for token counting, caching, semantic compression (using an LLM), and overall compression. The Compressor class orchestrates the compression process based on the specified compression level. It first checks the cache. If a cached result is found, it's used. Otherwise, it calls the appropriate compression method (semantic, smart, aggressive, medium, light, or none). The semantic compression uses an LLM to generate a compressed JSON representation of the file. The script also includes error handling and logging.\",\n  \"critical_reconstruction_details\": [\n    \"The token counting heuristic (3 chars = 1 token).\",\n    \"The prompt structure used for semantic compression, including the JSON output format.\",\n    \"The caching mechanism based on SHA-256 hashes.\",\n    \"The file type detection logic based on file extensions.\",\n    \"The handling of missing API keys and dependencies for semantic compression.\"\n  ],\n  \"external_interactions\": [\n    \"Interacts with the OpenRouter API for semantic compression (if enabled).\",\n    \"Loads environment variables from a .env file (optional).\",\n    \"Reads and writes to a cache file (JSON).\"\n  ]\n}",
      "original_size_bytes": 32540,
      "compressed_size_bytes": 6274,
      "estimated_tokens": 2091,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 5.186483901817023,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T07:50:40.085676Z"
    },
    {
      "path": "pak_differ.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak_differ.py (24042 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak_differ.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file manages the extraction and application of method-level diffs between Python files. It uses a PythonAnalyzer (from pak_analyzer.py) to compare methods and generate diff instructions, which can then be applied to modify target files.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"sys\",\n      \"typing: List, Dict, Any, Optional\",\n      \"pak_analyzer: PythonAnalyzer (from pak_analyzer.py)\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"MethodDiffManager\",\n        \"purpose\": \"Manages the extraction, conversion, parsing, and application of method diffs.\",\n        \"key_methods\": [\n          \"_log(message: str, quiet: bool, is_error: bool = False): Logs messages to stderr.\",\n          \"extract_diff(file_paths: List[str], quiet: bool = False) -> List[Dict[str, Any]]: Extracts method diffs between files.\",\n          \"_convert_to_diff_format(diff_detail: Dict[str, Any], modified_file_name: str, base_code_content: str, quiet: bool) -> Optional[Dict[str, Any]]: Converts PythonAnalyzer diff details to a structured diff format.\",\n          \"_find_next_definition_signature_in_text(file_text_content: str, start_search_after_line_num: int) -> str: Finds the next method/class definition signature in text.\",\n          \"_parse_diff_file(diff_file_path: str, quiet: bool = False) -> List[Dict[str, Any]]: Parses a .diff file into a list of diff instructions.\",\n          \"apply_diff(diff_file_path: str, target_base_path: str, quiet: bool = False) -> bool: Applies method diffs from a .diff file to target files.\",\n          \"_apply_single_instruction_to_file_content(instruction: Dict[str, Any], target_file_path: str, quiet: bool) -> bool: Applies a single diff instruction to a file.\",\n          \"verify_diff_file(diff_file_path: str, quiet: bool = False) -> bool: Verifies the syntax and basic structure of a .diff file.\"\n        ],\n        \"key_attributes\": []\n      }\n    ],\n    \"functions_methods\": [],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The `extract_diff` method takes a list of file paths, compares the methods in the files using `PythonAnalyzer.compare_methods`, and converts the diffs into a structured format. The `apply_diff` method parses a .diff file containing instructions, and then applies these instructions to a target file. The instructions include file name, find method signature, until exclude signature, and the replacement content. The `_parse_diff_file` method parses the .diff file format. The `_apply_single_instruction_to_file_content` method applies a single diff instruction to a file by finding the method signature, and replacing or removing the code block.\",\n  \"critical_reconstruction_details\": \"The `_convert_to_diff_format` method uses the `PythonAnalyzer`'s diff details and the base file content to determine the `until_exclude` signature, which is crucial for identifying the end of the method block to be replaced. The `_find_next_definition_signature_in_text` method is used to find the `until_exclude` signature. The .diff file format is custom and essential for storing and applying the diffs. The logic for handling 'added', 'modified', and 'removed' method types is critical.\",\n  \"external_interactions\": [\n    \"Interacts with the `pak_analyzer` module to perform method comparisons. Reads and writes files based on file paths provided as input. Reads and writes .diff files.\"\n  ]\n}",
      "original_size_bytes": 24042,
      "compressed_size_bytes": 3626,
      "estimated_tokens": 1208,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 6.630446773303916,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "pak_utils.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: pak_utils.py (6239 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"pak_utils.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file provides utility functions for collecting, filtering, validating, and analyzing files based on various criteria such as file extensions, patterns, and accessibility. It's designed to be used in file processing and management tasks.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"glob\",\n      \"pathlib\",\n      \"sys\",\n      \"typing\",\n      \"fnmatch\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"collect_files\",\n        \"signature\": \"collect_files(targets: list[str], extensions: list[str], quiet: bool = False) -> list[str]\",\n        \"purpose\": \"Collects files based on target paths (files, directories, globs) and filters by extensions. Handles directory traversal and glob pattern matching, including recursive globs.\"\n      },\n      {\n        \"name\": \"filter_files_by_pattern\",\n        \"signature\": \"filter_files_by_pattern(files: List[str], pattern: str) -> List[str]\",\n        \"purpose\": \"Filters a list of file paths based on a Unix shell-style wildcard pattern, matching against both the full path and the filename.\"\n      },\n      {\n        \"name\": \"validate_file_access\",\n        \"signature\": \"validate_file_access(file_paths: List[str], quiet: bool = False) -> List[str]\",\n        \"purpose\": \"Validates that files exist and are readable, returning a list of accessible file paths. Includes error handling and a quiet mode to suppress warnings.\"\n      },\n      {\n        \"name\": \"get_file_stats\",\n        \"signature\": \"get_file_stats(file_paths: List[str]) -> Dict[str, int]\",\n        \"purpose\": \"Calculates and returns statistics about a collection of files, including total file count, total size, file extensions, largest file size, and smallest file size.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The file defines several utility functions. `collect_files` is the core function, taking targets (files, directories, globs) and extensions as input, and returning a list of matching files. It uses `os.walk` for directory traversal and `glob` for pattern matching. `filter_files_by_pattern` filters a list of files based on a wildcard pattern. `validate_file_access` checks file existence and readability. `get_file_stats` computes file statistics. The `if __name__ == '__main__':` block provides example usage and testing.\",\n  \"critical_reconstruction_details\": \"The `collect_files` function uses `glob.iglob` for potentially large numbers of matches to save memory. It normalizes paths using `os.path.normpath`. Recursive globbing is handled by checking for '**' in the target pattern. The `get_file_stats` function handles potential `OSError` exceptions when accessing file sizes. The example usage includes creating and cleaning up test files and directories.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 6239,
      "compressed_size_bytes": 3043,
      "estimated_tokens": 1014,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 2.0502793296089385,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "python_extractor.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: python_extractor.py (3035 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"python_extractor.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This script extracts the structural elements (imports, classes, methods, functions, and constants) from a given Python file and outputs a simplified representation of its structure.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"re\",\n      \"sys\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"extract_python_structure\",\n        \"signature\": \"extract_python_structure(file_path)\",\n        \"purpose\": \"Extracts and formats the Python code structure from a given file path.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The script takes a file path as a command-line argument. It reads the file content, parses it line by line, and identifies import statements, class definitions, method definitions, function definitions, and constant assignments. It then formats these elements into a structured output, which is printed to the console.\",\n  \"critical_reconstruction_details\": \"The script uses regular expressions to identify class and method definitions. It also handles return type extraction from method signatures. The output format includes method signatures with return types (if present) and uses '...' to indicate method bodies are omitted. The script handles potential file reading errors.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 3035,
      "compressed_size_bytes": 1553,
      "estimated_tokens": 517,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 1.9542820347714103,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "semantic_compressor.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: semantic_compressor.py (8189 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"semantic_compressor.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This script semantically compresses code files using an LLM to generate concise descriptions, enabling reconstruction while reducing token count. It always applies semantic compression, regardless of file size or type.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"sys\",\n      \"os\",\n      \"json\",\n      \"logging\",\n      \"pathlib\",\n      \"typing\",\n      \"llm_wrapper\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"compress_file_semantically\",\n        \"signature\": \"compress_file_semantically(file_path: str, language: str, content: str) -> tuple[str, str]\",\n        \"purpose\": \"Compresses a file using LLM semantic analysis, returning the compressed content and a method description. Always applies semantic compression.\"\n      },\n      {\n        \"name\": \"decompress_semantic_file\",\n        \"signature\": \"decompress_semantic_file(compressed_content: str) -> Optional[str]\",\n        \"purpose\": \"Extracts the semantic description (JSON) from a compressed file, for reconstruction purposes.\"\n      },\n      {\n        \"name\": \"main\",\n        \"signature\": \"main()\",\n        \"purpose\": \"CLI interface for semantic compression, handling file input, compression level selection, and language specification.\"\n      }\n    ],\n    \"data_structures\": [\n      \"SEMANTIC_PROMPT_TEMPLATE: A string template used to construct the prompt for the LLM, defining the desired JSON output format.\",\n      \"Environment variables: PAK_DEBUG (for debug output)\"\n    ],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The script takes a file path, compression level ('semantic'), and language as input. It reads the file content, constructs a prompt for an LLM based on the SEMANTIC_PROMPT_TEMPLATE, and calls the LLM. The LLM's JSON response is then used to create a compressed file format. The script handles error conditions such as LLM failures and invalid JSON responses, providing fallback mechanisms.\",\n  \"critical_reconstruction_details\": \"The script relies on an external llm_wrapper.py for LLM interaction. The SEMANTIC_PROMPT_TEMPLATE defines the structure of the LLM's expected JSON output, which is crucial for reconstruction. The script includes logic to clean up and validate the LLM's JSON response, handling potential markdown blocks. The compression ratio is calculated and included in the method description. The decompress_semantic_file function provides the basic structure for extracting the semantic description.\",\n  \"external_interactions\": [\n    \"Interacts with an external LLM via llm_wrapper.py. Reads and writes files based on the provided file path. Optionally loads environment variables from a .env file.\"\n  ]\n}",
      "original_size_bytes": 8189,
      "compressed_size_bytes": 2900,
      "estimated_tokens": 966,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 2.823793103448276,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    },
    {
      "path": "test_pak_core_integration.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: test_pak_core_integration.py (13940 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"test_pak_core_integration.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file contains integration tests for the `pak.py` command-line tool. It tests the core functionality of `pak.py` by running it as a subprocess and verifying its behavior.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"shutil\",\n      \"subprocess\",\n      \"tempfile\",\n      \"json\",\n      \"pathlib\",\n      \"pytest (conditionally)\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"PakIntegrationTester\",\n        \"purpose\": \"Manages the test environment and runs integration tests for `pak.py`.\",\n        \"key_methods\": [\n          \"__init__(self): Initializes the test environment.\",\n          \"setup_test_environment(self): Creates a temporary directory and sample files for testing.\",\n          \"cleanup_test_environment(self): Removes the temporary test directory.\",\n          \"run_pak(self, args, input_data=None): Runs `pak.py` as a subprocess with given arguments and returns the result.\"\n        ],\n        \"key_attributes\": [\n          \"test_dir: Path to the temporary test directory.\",\n          \"pak_path: Path to the pak.py script.\"\n        ]\n      }\n    ],\n    \"functions_methods\": [\n      {\n        \"name\": \"pak_tester (pytest fixture)\",\n        \"signature\": \"@pytest.fixture\",\n        \"purpose\": \"Provides a `PakIntegrationTester` instance for each test function, setting up and tearing down the test environment.\"\n      },\n      {\n        \"name\": \"test_pak_core_pack_basic(pak_tester)\",\n        \"signature\": \"test_pak_core_pack_basic(pak_tester)\",\n        \"purpose\": \"Tests the basic pack functionality of `pak.py`.\"\n      },\n      {\n        \"name\": \"test_pak_core_pack_with_compression_levels(pak_tester)\",\n        \"signature\": \"test_pak_core_pack_with_compression_levels(pak_tester)\",\n        \"purpose\": \"Tests packing with different compression levels.\"\n      },\n      {\n        \"name\": \"test_pak_core_pack_with_extensions_filter(pak_tester)\",\n        \"signature\": \"test_pak_core_pack_with_extensions_filter(pak_tester)\",\n        \"purpose\": \"Tests packing with file extension filtering.\"\n      },\n      {\n        \"name\": \"test_pak_core_pack_with_max_tokens(pak_tester)\",\n        \"signature\": \"test_pak_core_pack_with_max_tokens(pak_tester)\",\n        \"purpose\": \"Tests packing with token limit.\"\n      },\n      {\n        \"name\": \"test_pak_core_list_archive(pak_tester)\",\n        \"signature\": \"test_pak_core_list_archive(pak_tester)\",\n        \"purpose\": \"Tests the list functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_list_detailed(pak_tester)\",\n        \"signature\": \"test_pak_core_list_detailed(pak_tester)\",\n        \"purpose\": \"Tests the detailed list functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_verify_archive(pak_tester)\",\n        \"signature\": \"test_pak_core_verify_archive(pak_tester)\",\n        \"purpose\": \"Tests the verify functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_extract_archive(pak_tester)\",\n        \"signature\": \"test_pak_core_extract_archive(pak_tester)\",\n        \"purpose\": \"Tests the extract functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_extract_with_pattern(pak_tester)\",\n        \"signature\": \"test_pak_core_extract_with_pattern(pak_tester)\",\n        \"purpose\": \"Tests the extract with pattern filtering functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_extract_diff(pak_tester)\",\n        \"signature\": \"test_pak_core_extract_diff(pak_tester)\",\n        \"purpose\": \"Tests the extract-diff functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_verify_diff(pak_tester)\",\n        \"signature\": \"test_pak_core_verify_diff(pak_tester)\",\n        \"purpose\": \"Tests the verify-diff functionality.\"\n      },\n      {\n        \"name\": \"test_pak_core_error_handling(pak_tester)\",\n        \"signature\": \"test_pak_core_error_handling(pak_tester)\",\n        \"purpose\": \"Tests error handling for invalid inputs.\"\n      },\n      {\n        \"name\": \"test_pak_core_quiet_mode(pak_tester)\",\n        \"signature\": \"test_pak_core_quiet_mode(pak_tester)\",\n        \"purpose\": \"Tests quiet mode functionality.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The file defines a `PakIntegrationTester` class to manage the test environment. It creates a temporary directory with sample files (Python, JavaScript, Markdown) and then uses `subprocess.run` to execute `pak.py` with various commands (pack, list, verify, extract, extract-diff, verify-diff). The tests assert the success of the commands and verify the results (e.g., archive creation, file contents, diff output). Pytest fixtures are used to manage the test environment setup and teardown.\",\n  \"critical_reconstruction_details\": \"The tests rely on creating a temporary directory and populating it with sample files to test the core functionality of `pak.py`. The tests use `subprocess.run` to execute `pak.py` and capture its output. The tests verify the output and the state of the file system to ensure the correct behavior of `pak.py`. The use of pytest fixtures for setup and teardown is crucial.\",\n  \"external_interactions\": [\n    \"Interacts with the file system to create and delete temporary directories and files.\",\n    \"Executes `pak.py` as a subprocess.\"\n  ]\n}",
      "original_size_bytes": 13940,
      "compressed_size_bytes": 5446,
      "estimated_tokens": 1815,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 2.559676827029012,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T07:13:32.795235Z"
    },
    {
      "path": "test_pak_integration.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: test_pak_integration.py (16938 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"test_pak_integration.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file contains integration tests for the `pak.py` CLI tool, verifying its functionality through various commands and scenarios, including packing, listing, extracting, and diff operations.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"os\",\n      \"shutil\",\n      \"subprocess\",\n      \"tempfile\",\n      \"json\",\n      \"sys\",\n      \"pathlib\",\n      \"pytest (conditionally)\"\n    ],\n    \"classes\": [\n      {\n        \"name\": \"PakIntegrationTester\",\n        \"purpose\": \"Manages the test environment and executes `pak.py` commands.\",\n        \"key_methods\": [\n          \"__init__(self): Initializes the test environment.\",\n          \"setup_test_environment(self): Creates a temporary test directory with sample files (calculator.py, calculator_modified.py, utils.js, README.md, config.json, helpers.py).\",\n          \"cleanup_test_environment(self): Removes the temporary test directory.\",\n          \"run_pak(self, args, input_data=None): Executes `pak.py` with given arguments and captures output.\"\n        ],\n        \"key_attributes\": [\n          \"test_dir: Path to the temporary test directory.\",\n          \"pak_script: Path to the pak.py script.\"\n        ]\n      }\n    ],\n    \"functions_methods\": [\n      {\n        \"name\": \"test_pak_version_and_help\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests the --help and version commands of pak.\"\n      },\n      {\n        \"name\": \"test_pak_basic_pack_shorthand_syntax\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests basic pack functionality with shorthand syntax.\"\n      },\n      {\n        \"name\": \"test_pak_compression_levels\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak with different compression levels.\"\n      },\n      {\n        \"name\": \"test_pak_list_commands\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak list commands.\"\n      },\n      {\n        \"name\": \"test_pak_extract_commands\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak extract commands.\"\n      },\n      {\n        \"name\": \"test_pak_verify_command\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak verify command.\"\n      },\n      {\n        \"name\": \"test_pak_method_diff_extract\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak method diff extraction.\"\n      },\n      {\n        \"name\": \"test_pak_method_diff_verify\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak method diff verification.\"\n      },\n      {\n        \"name\": \"test_pak_method_diff_apply\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak method diff application.\"\n      },\n      {\n        \"name\": \"test_pak_quiet_mode\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak quiet mode.\"\n      },\n      {\n        \"name\": \"test_pak_combined_flags\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak with combined shorthand flags.\"\n      },\n      {\n        \"name\": \"test_pak_error_handling\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak error handling.\"\n      },\n      {\n        \"name\": \"test_pak_dependency_checks\",\n        \"signature\": \"(pak_tester)\",\n        \"purpose\": \"Tests pak dependency checking.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The script defines a `PakIntegrationTester` class to manage a temporary test environment. It creates sample files and directories within this environment.  The script then defines several test functions, each of which calls `pak.py` with different arguments and asserts the results (return code, stdout, stderr) to verify the functionality of the pak CLI.  The tests cover various commands like packing, listing, extracting, diffing, and error handling.\",\n  \"critical_reconstruction_details\": \"The script uses `subprocess.run` to execute `pak.py` and capture its output. The tests rely on the creation of specific files and directories within the temporary test environment to simulate real-world scenarios. The tests use assertions to validate the behavior of `pak.py` based on its output and return codes. The test cases cover shorthand syntax, compression levels, list commands, extract commands, verify commands, method diff operations, quiet mode, combined flags, error handling, and dependency checks.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 16938,
      "compressed_size_bytes": 4588,
      "estimated_tokens": 1529,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 3.6918047079337404,
      "importance_score": 0,
      "last_modified_utc": "2025-06-19T07:14:12.378937Z"
    },
    {
      "path": "test_semantic_compressor.py",
      "content": "# SEMANTIC COMPRESSION v1.1 (pak_compressor.py)\n# Original: test_semantic_compressor.py (2075 bytes, python)\n# Model: google/gemini-2.0-flash-lite-001\n{\n  \"file_path\": \"test_semantic_compressor.py\",\n  \"file_type\": \"python\",\n  \"overall_purpose\": \"This file simulates semantic compression for testing cache functionality. It does not interact with real LLM APIs but generates a fake compressed output.\",\n  \"key_components\": {\n    \"imports_dependencies\": [\n      \"sys\",\n      \"hashlib\",\n      \"time\"\n    ],\n    \"classes\": [],\n    \"functions_methods\": [\n      {\n        \"name\": \"fake_semantic_compression\",\n        \"signature\": \"fake_semantic_compression(content, filename, language)\",\n        \"purpose\": \"Simulates semantic compression by creating a fake compressed result with a simulated delay. Generates a deterministic output based on the input content's hash.\"\n      },\n      {\n        \"name\": \"main\",\n        \"signature\": \"main()\",\n        \"purpose\": \"Handles command-line arguments, reads the input file, and calls the fake compression function. Prints the compressed output to standard output.\"\n      }\n    ],\n    \"data_structures\": [],\n    \"configuration\": []\n  },\n  \"core_logic_flow\": \"The script takes a file path, compression level, and language as command-line arguments. It reads the file content, simulates a delay, and then calls `fake_semantic_compression` to generate a fake compressed output. The output is printed to standard output. Error handling is included for file not found and incorrect compression level.\",\n  \"critical_reconstruction_details\": \"The `fake_semantic_compression` function uses `hashlib.md5` to create a hash of the input content, which is then used to generate a deterministic fake compression result. The output is a JSON-like string that simulates the structure of a semantic compression result. The simulated delay is implemented using `time.sleep(0.5)`.\",\n  \"external_interactions\": []\n}",
      "original_size_bytes": 2075,
      "compressed_size_bytes": 1930,
      "estimated_tokens": 643,
      "compression_method": "semantic-llm (cached)",
      "compression_ratio": 1.0751295336787565,
      "importance_score": 0,
      "last_modified_utc": "2025-06-18T17:56:15.087198Z"
    }
  ]
}